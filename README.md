# CONN: Coherence-Oscillator Neural Network

[![ResearchGate](https://img.shields.io/badge/ResearchGate-Preprint-00CCBB.svg)](https://www.researchgate.net/profile/Labinot-Marku)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Data License: CC BY 4.0](https://img.shields.io/badge/Data%20License-CC%20BY%204.0-lightgrey.svg)](https://creativecommons.org/licenses/by/4.0/)

> **âš ï¸ Repository Status:** Under active development. Manuscript available on ResearchGate. Code and data being progressively added.

**Topological Phase Constraints and Amplitude Dynamics Improve Associative Memory in Oscillatory Neural Networks**

**Author:** Labinot Marku, M.D.  
**Affiliation:** KRH Klinikum Nordstadt Hannover, Department of Neurosurgery  
**Address:** Haltenhoffstr. 41, 30167 Hannover, Germany  
**Contact:** labinot.marku@krh.de  
**Date:** December 21, 2025

---

## ğŸ¯ TL;DR

CONN augments classical Hopfield networks with oscillatory dynamics and topological priors, achieving **1.38Ã— average capacity improvement** (ranging from 1.09Ã— to 1.69Ã— across network sizes N=32-128). All results are fully reproducible.

**Key Results:**
- ğŸ“ˆ **1.38Ã— capacity improvement** over Hopfield networks
- ğŸ”¬ **Rigorous validation:** 1,920 trials with 30 replications per condition
- ğŸ§  **Biologically inspired:** Phase-amplitude coupling observed in cortical networks
- âš¡ **Scaling behavior:** Improvement increases with network size (1.69Ã— at N=128)
- ğŸ”“ **Fully reproducible:** Complete methodology documented

---

## ğŸ“Š Quick Results

| Network Size | CONN Capacity | Hopfield Capacity | Improvement |
|--------------|---------------|-------------------|-------------|
| N=32         | Î± = 0.375     | Î± = 0.344         | **1.09Ã—**   |
| N=64         | Î± = 0.359     | Î± = 0.266         | **1.35Ã—**   |
| N=128        | Î± = 0.383     | Î± = 0.227         | **1.69Ã—**   |

*Using optimized hyperparameters (Î»=1.0, Î²=0.5) at 30% noise*

---

## ğŸ“„ Manuscript

The complete manuscript with detailed methods, results, and analysis is available on ResearchGate:

**[View Manuscript on ResearchGate â†’](https://www.researchgate.net/profile/Labinot-Marku)**

### Abstract

We introduce CONN (Coherence-Oscillator Neural Network), a phase-amplitude recurrent architecture that augments classical Hopfield associative memory using a Ï€-coherence topological prior and amplitude dynamics that implement implicit Bayesian pruning. Through systematic hyperparameter optimization and rigorous validation, we demonstrate that CONN achieves a reproducible 1.38Ã— average capacity improvement over classical Hopfield networks, with performance gains scaling from 1.09Ã— (N=32) to 1.69Ã— (N=128).

---

## ğŸ”¬ Model Overview

CONN represents each neuron as a **phase-amplitude oscillator**:

```
z_j = A_j Ã— e^(iÏ†_j)
where A_j â‰¥ 0, Ï†_j âˆˆ [0, 2Ï€)
```

### Energy Function

```
E = -(1/2) Î£ J_ij A_i A_j cos(Ï†_i - Ï†_j) + Î» Î£ A_jÂ² sinÂ²(Ï†_j) + Î² Î£ (A_j - 1)Â²
```

**Key innovation:** The Ï€-coherence term `Î» Î£ A_jÂ² sinÂ²(Ï†_j)` provides a topological prior that biases phases toward {0, Ï€}, reducing spurious attractors.

### Dynamics

**Phase:**
```
dÏ†_j/dt = A_j Î£ J_ij A_i sin(Ï†_i - Ï†_j) - 2Î»A_jÂ² sin(Ï†_j)cos(Ï†_j)
```

**Amplitude:**
```
dA_j/dt = -2Î»A_j sinÂ²(Ï†_j) - 2Î²(A_j - 1)
```

**Optimized parameters:** Î»=1.0, Î²=0.5, Î·_Ï†=0.005, Î·_A=0.03

---

## ğŸ“ˆ Key Findings

### 1. Capacity Improvement Scales with Network Size

```
Improvement Ratio
1.8 |                              â—
1.6 |                         â—
1.4 |                    â—
1.2 |              â—
1.0 |__________â—________________________
    0    32   64   96  128  160  192
              Network Size (N)
```

**Interpretation:** Topological priors become more effective in higher-dimensional spaces.

### 2. Hyperparameter Optimization is Critical

| Î» (coherence) | Capacity Î± (N=32) | vs Optimal |
|---------------|-------------------|------------|
| 1.0 (optimal) | 0.375             | â€”          |
| 2.0           | 0.344             | âˆ’8%        |
| 4.0 (initial) | 0.250             | âˆ’33%       |
| 8.0           | 0.281             | âˆ’25%       |

**Lesson:** Too-strong coherence (Î»>4) over-constrains dynamics and hurts performance.

### 3. Ablation Study

| Configuration            | Recall (N=32, M=8) |
|--------------------------|---------------------|
| Full CONN (Î»=1.0)        | 85.3%               |
| No coherence (Î»=0)       | 72.1%               |
| No amplitude dynamics    | 80.9%               |
| Baseline (Î»=0, no amp)   | 68.5%               |

Ï€-coherence contributes +13.2 percentage points, amplitude dynamics +4.4 pp.

---

## ğŸ—ºï¸ Repository Roadmap

### âœ… Completed
- [x] Manuscript published on ResearchGate
- [x] Repository structure defined
- [x] README documentation

### ğŸš§ In Progress
- [ ] Core CONN implementation (`src/conn_core.py`)
- [ ] Validation suite (`src/conn_final_validation.tsx`)
- [ ] Hyperparameter search code (`src/hyperparameter_search.tsx`)
- [ ] Hopfield baseline (`src/hopfield_baseline.py`)

### ğŸ“‹ Planned
- [ ] Raw experimental data (1,920 trials CSV)
- [ ] Hyperparameter search results (720 trials CSV)
- [ ] Figure generation scripts
- [ ] Jupyter notebooks for analysis
- [ ] Installation and reproduction instructions
- [ ] Unit tests

---

## ğŸ“ Planned Repository Structure

```
CONN-Associative-Memory/
â”œâ”€â”€ README.md                          # This file
â”œâ”€â”€ requirements.txt                   # Python dependencies (coming soon)
â”œâ”€â”€ LICENSE                            # MIT License
â”‚
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ manuscript.pdf                 # Complete manuscript
â”‚   â””â”€â”€ supplementary_materials.pdf    # Supplementary materials
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ conn_core.py                  # Core CONN implementation
â”‚   â”œâ”€â”€ conn_final_validation.tsx     # Interactive validation suite
â”‚   â”œâ”€â”€ hyperparameter_search.tsx     # Parameter optimization
â”‚   â””â”€â”€ hopfield_baseline.py          # Hopfield comparison
â”‚
â”œâ”€â”€ experiments/
â”‚   â”œâ”€â”€ run_full_validation.py        # Main validation
â”‚   â”œâ”€â”€ run_hyperparameter_search.py  # Optimization
â”‚   â”œâ”€â”€ run_ablation.py               # Ablation study
â”‚   â””â”€â”€ run_noise_robustness.py       # Noise analysis
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                           # Raw trial data (CSV)
â”‚   â””â”€â”€ processed/                     # Summary statistics
â”‚
â”œâ”€â”€ figures/                           # All figures (PDF)
â”‚
â””â”€â”€ analysis/
    â”œâ”€â”€ statistical_analysis.ipynb    # Jupyter notebook
    â”œâ”€â”€ generate_figures.py           # Reproduce figures
    â””â”€â”€ verify_reproducibility.py     # Verification
```

---

## ğŸ› ï¸ Installation (Coming Soon)

Once code is uploaded, installation will be:

```bash
# Clone repository
git clone https://github.com/labinot-marku/CONN-Associative-Memory.git
cd CONN-Associative-Memory

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

---

## ğŸ“– Usage (Preview)

Once implemented, typical usage will be:

```python
from src.conn_core import CONN

# Initialize network
conn = CONN(N=32, lambda_param=1.0, beta=0.5)

# Store patterns
patterns = [[0, Ï€, 0, Ï€, ...], ...]  # Binary phase patterns
conn.store_patterns(patterns)

# Test recall
noisy_pattern = add_noise(patterns[0], noise_level=0.3)
recovered = conn.recall(noisy_pattern, steps=150)

print(f"Overlap: {compute_overlap(recovered, patterns[0]):.2%}")
```

---

## ğŸ“„ Citation

If you use this work in your research, please cite:

```bibtex
@misc{marku2025conn,
  title={Topological Phase Constraints and Amplitude Dynamics Improve 
         Associative Memory in Oscillatory Neural Networks},
  author={Marku, Labinot},
  year={2025},
  institution={KRH Klinikum Nordstadt Hannover},
  note={Preprint available on ResearchGate}
}
```

---

## ğŸ¤ Contributing

This is an active research project. Contributions are welcome:

- **Bug reports:** Open an issue if you find errors
- **Reproductions:** Share your reproduction results
- **Extensions:** Propose improvements or extensions
- **Discussion:** Open issues for scientific discussion

Please wait for initial code upload before submitting pull requests.

---

## ğŸ“œ License

- **Code:** MIT License (when uploaded)
- **Data & Figures:** CC BY 4.0
- **Manuscript:** CC BY 4.0

You are free to use, modify, and distribute this work with attribution.

---

## ğŸ™ Acknowledgments

- Inspired by biological phase-amplitude coupling in cortical networks
- Built on foundational work by Hopfield (1982), Krotov & Hopfield (2016)
- Validation methodology influenced by modern reproducibility practices

---

## ğŸ“§ Contact

**Labinot Marku, M.D.**  
KRH Klinikum Nordstadt Hannover  
Department of Neurosurgery  
Haltenhoffstr. 41  
30167 Hannover, Germany

ğŸ“§ Email: labinot.marku@krh.de  
ğŸ™ GitHub: [@labinot-marku](https://github.com/labinot-marku)  
ğŸ”¬ ResearchGate: [Profile](https://www.researchgate.net/profile/Labinot-Marku)

**For questions:**
- Technical issues: Open a GitHub issue
- Scientific discussion: Email or ResearchGate message
- Collaboration inquiries: Email directly

---

## ğŸ“š Additional Resources

- **Manuscript:** [ResearchGate](https://www.researchgate.net/profile/Labinot-Marku)
- **Supplementary Materials:** Available upon request
- **Interactive Demo:** Coming soon

---

## ğŸ”– Keywords

`associative-memory` `hopfield-networks` `neural-oscillations` `phase-amplitude-coupling` `computational-neuroscience` `machine-learning` `topological-priors` `capacity-bounds` `reproducible-research` `neurosurgery-research`

---

## ğŸ“Š Development Status

**Last updated:** December 21, 2025  
**Version:** 0.1.0-dev  
**Status:** Manuscript complete, code upload in progress  
**Preprint:** Available on ResearchGate (not peer-reviewed)

---

**Made with â¤ï¸ for open and reproducible science**

---

## ğŸ”” Updates

Subscribe to repository updates to be notified when:
- Source code is uploaded
- Data becomes available
- Analysis scripts are added
- Interactive demos launch

**Star â­ this repository** to follow progress!
